{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A contingency matrix, often referred to as a confusion matrix, is a table used to evaluate the performance of a classification model. It compares the actual target values with the model's predictions. Each row of the matrix represents instances in a predicted class, while each column represents instances in an actual class (or vice versa).\n",
    "\n",
    "# For binary classification, the matrix typically includes:\n",
    "# True Positives (TP): Correctly predicted positive class.\n",
    "# True Negatives (TN): Correctly predicted negative class.\n",
    "# False Positives (FP): Incorrectly predicted as positive.\n",
    "# False Negatives (FN): Incorrectly predicted as negative.\n",
    "\n",
    "# From this matrix, you can derive metrics like accuracy, precision, recall, F1-score, etc., helping identify how well the model is performing and where it might be going wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in\n",
    "certain situations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pair confusion matrix is primarily used for evaluating clustering and unsupervised learning results, particularly in scenarios where labels are not available. It compares all pairs of samples to determine if they are assigned to the same or different clusters, and whether that matches the ground truth grouping (if available).\n",
    "\n",
    "# The matrix includes:\n",
    "# True Positive (TP): Pairs that are in the same cluster and same class.\n",
    "# False Positive (FP): Same cluster, different class.\n",
    "# False Negative (FN): Different clusters, same class.\n",
    "# True Negative (TN): Different clusters, different class.\n",
    "\n",
    "# It is useful in clustering or multi-label problems where evaluating individual predictions is not enough, and you care more about the relationships between instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically\n",
    "used to evaluate the performance of language models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An extrinsic measure evaluates a model based on its performance in a real-world task. In NLP, this means assessing how well a language model performs when integrated into downstream applications such as:\n",
    "\n",
    "# Text summarization\n",
    "# Machine translation\n",
    "# Question answering\n",
    "# Sentiment analysis\n",
    "# For instance, evaluating a language model on BLEU score for translation, or accuracy/F1 score in a classification task, are examples of extrinsic evaluation. These measures reflect task-specific utility rather than just model architecture or internal quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an\n",
    "extrinsic measure?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An intrinsic measure evaluates a model's performance based on its internal behavior or output, without using an external task. In machine learning and NLP, intrinsic evaluations are focused on assessing the model’s structure or intermediate outputs.\n",
    "\n",
    "# Examples:\n",
    "# Perplexity for language models\n",
    "# Coherence in topic models\n",
    "# Cosine similarity in word embeddings\n",
    "# The difference is that intrinsic measures evaluate the model directly, while extrinsic measures evaluate it indirectly via performance on a downstream task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify\n",
    "strengths and weaknesses of a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix serves as a detailed breakdown of the classification results. It helps in:\n",
    "\n",
    "# Identifying the types of errors a model is making (e.g., false positives vs. false negatives)\n",
    "\n",
    "# Calculating derived metrics like precision, recall, specificity, and F1-score\n",
    "\n",
    "# Highlighting class imbalance issues\n",
    "\n",
    "# By examining the matrix, we can see whether the model is biased toward a certain class or struggles to differentiate between similar classes. This allows targeted improvements in model training or data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised\n",
    "learning algorithms, and how can they be interpreted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common intrinsic measures for unsupervised learning (especially clustering) include:\n",
    "\n",
    "# Silhouette Score: Measures how similar an object is to its own cluster compared to other clusters. Range: -1 to 1. Higher is better.\n",
    "\n",
    "# Davies–Bouldin Index: Measures average similarity ratio of each cluster with the one most similar to it. Lower is better.\n",
    "\n",
    "# Calinski–Harabasz Index: Ratio of between-cluster dispersion to within-cluster dispersion. Higher is better.\n",
    "\n",
    "# These metrics evaluate the quality of clustering without needing ground truth labels, and help determine the optimal number of clusters or validate clustering strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and\n",
    "how can these limitations be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limitations of accuracy:\n",
    "# Misleading with imbalanced datasets: A model predicting only the majority class can still have high accuracy.\n",
    "# Doesn’t distinguish between error types: False positives and false negatives are not separately analyzed.\n",
    "# Insensitive to performance variation across classes\n",
    "\n",
    "# How to address these:\n",
    "# Use additional metrics: Precision, Recall, F1-score\n",
    "# For multi-class tasks, use macro/micro-averaged metrics\n",
    "# For imbalance, use ROC-AUC, PR curves, or balanced accuracy\n",
    "# These metrics provide a more comprehensive view of model performance, especially in real-world, imbalanced, or critical classification scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
