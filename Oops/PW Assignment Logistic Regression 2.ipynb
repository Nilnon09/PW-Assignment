{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search Cross-Validation (GridSearchCV):\n",
    "\n",
    "# Purpose: To find the best combination of hyperparameters for a model.\n",
    "\n",
    "# How it works:\n",
    "\n",
    "# Grid search exhaustively tests all possible combinations of hyperparameters within a predefined search space.\n",
    "\n",
    "# For each combination, it evaluates model performance using cross-validation, typically k-fold cross-validation.\n",
    "\n",
    "# It returns the hyperparameters that result in the best model performance (often based on metrics like accuracy, AUC, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 5, 'n_estimators': 10}\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load sample dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [5, 10, 15]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Test Accuracy:\", grid_search.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search CV:\n",
    "# Exhaustive Search: Tests all possible combinations of hyperparameters within the specified grid.\n",
    "# Pros: Guaranteed to find the optimal hyperparameter combination within the grid.\n",
    "# Cons: Computationally expensive if the grid is large.\n",
    "\n",
    "# Randomized Search CV:\n",
    "# Random Sampling: Randomly samples from the hyperparameter space for a fixed number of iterations.\n",
    "# Pros: Faster than grid search because it doesn't test every possible combination.\n",
    "# Cons: May not find the optimal combination, but often gives a good approximation.\n",
    "\n",
    "# When to choose:\n",
    "# Grid Search: When the hyperparameter space is small and you want an exhaustive search.\n",
    "# Randomized Search: When the hyperparameter space is large or you want faster results with less computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Leakage:\n",
    "# Occurs when information from outside the training dataset is used to create the model, leading to overly optimistic performance.\n",
    "# Problem: It can cause the model to perform well on training data but fail in real-world scenarios because the model has access to future information that wouldn't be available during prediction.\n",
    "\n",
    "# Example:\n",
    "# In a credit scoring model, if future financial transactions are included in the model as features, the model might \"leak\" future information about the applicant, which is unrealistic in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevention Methods:\n",
    "\n",
    "# Separate Data: Ensure that the training and test datasets are strictly separated, and no information from the test set is used during training.\n",
    "\n",
    "# Feature Engineering: Carefully choose features to avoid using information from future events or data points that wouldn't be available during prediction.\n",
    "\n",
    "# Proper Cross-Validation: Use cross-validation where the test data is not involved in any part of model training or hyperparameter tuning.\n",
    "\n",
    "# Temporal Validation: For time-series data, ensure the model is trained on past data only, without future information leaking in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix:\n",
    "# A table that compares the actual labels to the predicted labels, summarizing the performance of a classification model.\n",
    "\n",
    "# It contains four values:\n",
    "# True Positives (TP): Correct positive predictions.\n",
    "# True Negatives (TN): Correct negative predictions.\n",
    "# False Positives (FP): Incorrect positive predictions (Type I error).\n",
    "# False Negatives (FN): Incorrect negative predictions (Type II error).\n",
    "\n",
    "# It tells you:\n",
    "# The model’s overall performance.\n",
    "# How well the model differentiates between classes (positives and negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision:\n",
    "# Definition: The proportion of positive predictions that are actually correct.\n",
    "# Precision = TP / (TP + FP)\n",
    "#Use case: When you want to minimize false positives (e.g., in spam email detection).\n",
    "\n",
    "#Recall (Sensitivity):\n",
    "#Definition: The proportion of actual positives that are correctly identified.\n",
    "# Recall = TP/ (TP + FN)\n",
    "#Use case: When you want to minimize false negatives (e.g., in medical diagnoses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting Errors:\n",
    "# False Positives (FP): The model incorrectly classifies a negative instance as positive. This is a Type I error.\n",
    "# Example: Predicting a non-cancerous patient as having cancer.\n",
    "\n",
    "# False Negatives (FN): The model incorrectly classifies a positive instance as negative. This is a Type II error.\n",
    "# Example: Predicting a cancerous patient as cancer-free.\n",
    "\n",
    "# By examining these errors, you can adjust your model’s decision threshold, feature selection, or even the data to reduce certain types of errors (depending on the problem context)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy = TP + TN / (TP + TN + FP + FN)\n",
    "\n",
    "#Precision = TP / (TP + FP)\n",
    "\n",
    "#Recall = TP / (TP + FN)\n",
    "\n",
    "#f1-score \n",
    "#The harmonic mean of precision and recall:\n",
    "#f1-score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "#specificity (true negative rate):\n",
    "#specificity = TN/(TN + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy:\n",
    "\n",
    "# Accuracy is simply the proportion of correct predictions (both TP and TN) to the total number of samples.\n",
    "#Accuracy = TP + TN / (TP + TN + FP + FN)\n",
    "\n",
    "# Relationship:\n",
    "\n",
    "# While accuracy provides a general sense of performance, it can be misleading, especially in imbalanced datasets, because it doesn't take into account the distribution of FP and FN.\n",
    "\n",
    "# In cases where one class dominates, accuracy might be high even if the model is not correctly predicting the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Bias or Limitations:\n",
    "\n",
    "# High False Positive Rate (FP): Indicates the model is wrongly classifying many negative cases as positive. This could be due to class imbalance or an inappropriate decision threshold.\n",
    "# High False Negative Rate (FN): Suggests the model is missing many positive cases. This could point to issues with the model's sensitivity or insufficient feature engineering.\n",
    "# Class Imbalance: A model might predict the majority class well but fail to recognize the minority class (leading to a high accuracy but poor recall for the minority class). Adjustments like resampling or class weighting can help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
