{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaec99cd",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a1f52",
   "metadata": {},
   "source": [
    "Polynomial functions are non-linear transformations of input features that map data to a higher-dimensional space. This is useful when the data is not linearly separable.\n",
    "\n",
    "In SVMs, rather than explicitly transforming data into a higher dimension, we use the kernel trick.\n",
    "\n",
    " Polynomial Kernel Function:\n",
    "ùêæ\n",
    "(\n",
    "ùë•\n",
    ",\n",
    "ùë•\n",
    "‚Ä≤\n",
    ")\n",
    "=(\n",
    "ùõæ\n",
    "‚ãÖ\n",
    "ùë•\n",
    "ùëá\n",
    "ùë•\n",
    "‚Ä≤\n",
    "+\n",
    "ùëü\n",
    ")\n",
    "ùëë\n",
    "K(x,x \n",
    "‚Ä≤\n",
    " )=(Œ≥‚ãÖx \n",
    "T\n",
    " x \n",
    "‚Ä≤\n",
    " +r) \n",
    "d\n",
    " \n",
    "This behaves like computing a polynomial transformation implicitly, saving time and computation. Hence, polynomial kernels simulate non-linear decision boundaries without explicitly adding polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34944ec",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253c7d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96        52\n",
      "           1       0.97      0.92      0.95        38\n",
      "\n",
      "    accuracy                           0.96        90\n",
      "   macro avg       0.96      0.95      0.95        90\n",
      "weighted avg       0.96      0.96      0.96        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Sample data\n",
    "X, y = make_classification(n_samples=300, n_features=4, random_state=42)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# SVM with polynomial kernel\n",
    "clf = SVC(kernel='poly', degree=3, gamma='scale', C=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prediction and Evaluation\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c945eaf",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c88f0eb",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), epsilon defines a margin of tolerance around the true value where no penalty is given.\n",
    "\n",
    "What happens when you increase epsilon?\n",
    "\n",
    "The tube around the regression line widens.\n",
    "\n",
    "More training points fall inside the epsilon zone, leading to fewer support vectors.\n",
    "\n",
    "The model becomes less sensitive and may underfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7df4bc",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9550bce9",
   "metadata": {},
   "source": [
    "üîπ Kernel\n",
    "Chooses the shape of the regression function.\n",
    "\n",
    "Linear: For simple trends.\n",
    "\n",
    "RBF: Captures non-linear relationships.\n",
    "\n",
    "Polynomial: Good for smooth, curve-fitting problems.\n",
    "\n",
    "üîπ C (Regularization)\n",
    "High C: Low tolerance for errors ‚Üí overfitting.\n",
    "\n",
    "Low C: High tolerance ‚Üí underfitting.\n",
    "\n",
    "üîπ Epsilon\n",
    "Wider Œµ tube ‚Üí simpler model, less overfitting.\n",
    "\n",
    "Narrow Œµ ‚Üí more sensitive fit (more support vectors).\n",
    "\n",
    "üîπ Gamma (RBF/Poly kernels)\n",
    "High gamma: Points have close influence ‚Üí overfit.\n",
    "\n",
    "Low gamma: Points have far influence ‚Üí underfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2605d70f",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "L Import the necessary libraries and load the dataseg\n",
    "L Split the dataset into training and testing setZ\n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normalizationK\n",
    "L Create an instance of the SVC classifier and train it on the training datW\n",
    "L Use the trained classifier to predict the labels of the testing datW\n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to\n",
    "improve its performanc_\n",
    "L Train the tuned classifier on the entire dataseg\n",
    "L Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d6b7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tuning:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       0.95      1.00      0.98        21\n",
      "           2       1.00      0.93      0.96        14\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n",
      "Best Parameters: {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "‚úÖ Model and Scaler saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Step 2: Load Dataset\n",
    "data = load_wine()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Step 3: Train-test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Preprocess (Scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 5: Create and Train SVC\n",
    "svc = SVC(kernel='rbf', C=1, gamma='scale')\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 6: Predict and Evaluate\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "print(\"Before tuning:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 7: Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "    'kernel': ['rbf', 'poly']\n",
    "}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Step 8: Train on Full Dataset\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(scaler.fit_transform(X), y)  # Retrain on entire dataset\n",
    "\n",
    "# Step 9: Save Model\n",
    "joblib.dump(best_model, 'best_svm_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "print(\"‚úÖ Model and Scaler saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8eba9a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
