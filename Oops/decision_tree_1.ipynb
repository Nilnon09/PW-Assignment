{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f8bad6f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  **Q1. Describe the Decision Tree Classifier Algorithm and How it Works to Make Predictions**\n",
    "\n",
    "A **Decision Tree Classifier** is a supervised machine learning algorithm used for **classification tasks**. It works by splitting the data into subsets based on the **feature values**, in a tree-like structure.\n",
    "\n",
    "**How it works:**\n",
    "1. Starts with the entire dataset (root).\n",
    "2. Selects the best feature to split the data using metrics like **Gini Impurity** or **Information Gain (Entropy)**.\n",
    "3. Recursively splits the data into branches until:\n",
    "   - All data points in a node belong to the same class.\n",
    "   - Or, maximum depth / minimum samples per leaf is reached.\n",
    "4. At prediction time, new data follows the path from root to leaf and is assigned the label of the leaf.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca3724c",
   "metadata": {},
   "source": [
    "\n",
    "###  **Q2. Mathematical Intuition Behind Decision Tree Classification**\n",
    "\n",
    "**Key metrics:**\n",
    "\n",
    "1. **Entropy** (information gain based):\n",
    "   \\[\n",
    "   Entropy(S) = -p_+ \\log_2(p_+) - p_- \\log_2(p_-)\n",
    "   \\]\n",
    "   where \\( p_+ \\) is the proportion of positive samples, \\( p_- \\) is the proportion of negative samples.\n",
    "\n",
    "2. **Information Gain**:\n",
    "   \\[\n",
    "   IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\cdot Entropy(S_v)\n",
    "   \\]\n",
    "\n",
    "3. **Gini Impurity** (used by CART algorithm):\n",
    "   \\[\n",
    "   Gini(S) = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "   \\]\n",
    "\n",
    "The algorithm chooses the split that **minimizes impurity or maximizes information gain**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b8257b",
   "metadata": {},
   "source": [
    "###  **Q3. Decision Tree for Binary Classification**\n",
    "\n",
    "For binary classification (e.g., Yes/No, 0/1, Spam/Not Spam):\n",
    "\n",
    "1. At each node, choose the feature and threshold that best separates the two classes.\n",
    "2. Recursively apply this until each leaf is **pure** (only 0s or 1s).\n",
    "3. During prediction, input follows the decision path to a leaf, which contains the predicted class.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9aa619",
   "metadata": {},
   "source": [
    "###  **Q4. Geometric Intuition Behind Decision Trees**\n",
    "\n",
    "- Decision trees **split the feature space into axis-aligned rectangles**.\n",
    "- Each internal node defines a **hyperplane (parallel to axis)** that splits the space.\n",
    "- The result is a **piecewise constant function**, assigning the same class to all points in a region.\n",
    "- Not smooth or curved like SVMs or neural nets.\n",
    "\n",
    "Think of it like: *cutting the feature space like slicing a cake with straight vertical or horizontal cuts*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9596e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6eabec33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Q5. Define the Confusion Matrix**\n",
    "\n",
    "A **Confusion Matrix** is a table used to evaluate the performance of a classification algorithm.\n",
    "\n",
    "|                | Predicted Positive | Predicted Negative |\n",
    "|----------------|--------------------|--------------------|\n",
    "| **Actual Positive** | True Positive (TP)   | False Negative (FN)  |\n",
    "| **Actual Negative** | False Positive (FP)  | True Negative (TN)   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e679e06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad387e54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###  **Q6. Example & Metrics Calculation**\n",
    "\n",
    "Example Confusion Matrix:\n",
    "\n",
    "|                | Predicted Positive | Predicted Negative |\n",
    "|----------------|--------------------|--------------------|\n",
    "| **Actual Positive** | 80                 | 20                 |\n",
    "| **Actual Negative** | 10                 | 90                 |\n",
    "\n",
    "- **Precision**:  \n",
    "  \\[\n",
    "  \\text{Precision} = \\frac{TP}{TP + FP} = \\frac{80}{80 + 10} = 0.888\n",
    "  \\]\n",
    "\n",
    "- **Recall**:  \n",
    "  \\[\n",
    "  \\text{Recall} = \\frac{TP}{TP + FN} = \\frac{80}{80 + 20} = 0.80\n",
    "  \\]\n",
    "\n",
    "- **F1 Score**:  \n",
    "  \\[\n",
    "  F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall} = 2 \\cdot \\frac{0.888 \\cdot 0.80}{0.888 + 0.80} \\approx 0.842\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae163e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c86d14c8",
   "metadata": {},
   "source": [
    "###  **Q7. Choosing the Right Evaluation Metric**\n",
    "\n",
    "The choice of metric depends on the **business problem** and the **cost of errors**:\n",
    "\n",
    "- **Accuracy**: Good if classes are balanced and all errors cost the same.\n",
    "- **Precision**: Important when **false positives** are more costly.\n",
    "- **Recall**: Important when **false negatives** are more costly.\n",
    "- **F1 Score**: A balanced measure when both FP and FN matter.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c3a8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cea6ad6",
   "metadata": {},
   "source": [
    "\n",
    "### \n",
    " **Q8. Example Where Precision is More Important**\n",
    "\n",
    "**Spam Detection:**\n",
    "- False positives (marking a legitimate email as spam) are more problematic.\n",
    "- Want high precision — if we say \"spam,\" it should really be spam.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6abd6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a64d604e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###  **Q9. Example Where Recall is More Important**\n",
    "\n",
    "**Disease Diagnosis (e.g., Cancer Screening):**\n",
    "- False negatives (missing a cancer case) are critical.\n",
    "- Want high recall — we want to catch all real positive cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d53e89e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fb23d6e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
