{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach?\n",
    "Explain with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition approach? Explain with an example.\n",
    "\n",
    "# Eigenvalues are scalar values that represent how much a corresponding eigenvector is stretched or compressed during a linear transformation. \n",
    "# An eigenvector is a non-zero vector that only gets scaled (stretched or compressed), rather than rotated, when a linear transformation is applied to it.\n",
    "\n",
    "# Eigenvectors are vectors that remain in the same direction after a linear transformation, only changing in magnitude (scaling). \n",
    "# They are the directions along which a transformation acts by scaling rather than rotating.\n",
    "\n",
    "# The Eigen-Decomposition approach decomposes a matrix  into a product of three matrices:\n",
    "#  A = V A V^-1 \n",
    "#where :\n",
    "#V is the matrix of eigenvectors\n",
    "#A is the diagonal matrix of eigenvalues\n",
    "#V^-1 is the inverse of the matrix of eigenvectors\n",
    "\n",
    "#example :\n",
    "\n",
    "#consider the matrix A = [[4,1],[2,3]]\n",
    "\n",
    "#to find the eigenvalues(位) and eigenvalues(v), we solve the characterstics eqn:\n",
    "#det|A - 位I| = 0\n",
    "#where I is the identity matrix.\n",
    "#Solving this equation will give us the eigenvalues, and we can then find the eigenvectors associated with each eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is eigen decomposition and what is its significance in linear algebra?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigen-Decomposition is the process of decomposing a square matrix into its eigenvalues and eigenvectors. \n",
    "# It is significant because it provides a way to express a matrix as a sum of simpler, diagonalizable components. \n",
    "# The decomposition is: A = V *  * V^(-1) \n",
    "\n",
    "# Where \n",
    "# A is the matrix, \n",
    "# V is the matrix of eigenvectors, and \n",
    "#  is the diagonal matrix of eigenvalues.\n",
    "\n",
    "# Eigen-decomposition is crucial in many areas of linear algebra and has applications in:\n",
    "\n",
    "# Simplifying matrix powers and exponentiation.\n",
    "\n",
    "# Solving differential equations.\n",
    "\n",
    "# Finding matrix inverses for diagonalizable matrices.\n",
    "\n",
    "# Its particularly helpful in diagonalizing a matrix, making it easier to manipulate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the\n",
    "Eigen-Decomposition approach? Provide a brief proof to support your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A sqaure matrix A is A is diagonalizable if it has a full set of linearly independent eigenvectors. \n",
    "# This means the matrix A must have n linearly independent eigenvectors, where\n",
    "#n is the dimension of the matrix\n",
    "\n",
    "# Conditions:\n",
    "\n",
    "# The matrix must have n distinct eigenvalues (for an n * n matrix)  ensuring that there are enough linearly independent eigenvectors to form a matrix V.\n",
    "#If a matrix has repeated eigenvalues, it is still diagonalizable if there are enough linearly independent eigenvectors corresponding to those eigenvalues.\n",
    "\n",
    "\n",
    "# Proof Outline: If a matrix has  linearly independent eigenvectors, we can form a matrix V whose columns are the eigenvectors. \n",
    "# The inverse of V exists, and the matrix A can be expressed as:\n",
    "#     A = V * D * V^(-1)\n",
    "    \n",
    "# This proves that the matrix is diagonalizable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach?\n",
    "How is it related to the diagonalizability of a matrix? Explain with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Spectral Theorem states that any symmetric matrix can be diagonalized by an orthogonal matrix, meaning it can be written as:\n",
    "# A = QQ^(T)\n",
    "#Where Q is an orthogonal matrix (its inverse is its transpose), and  is a diagonal matrix of eigenvalues.\n",
    "\n",
    "# Significance:\n",
    "# It provides a way to diagonalize symmetric matrices using orthogonal eigenvectors.\n",
    "# It guarantees that symmetric matrices always have real eigenvalues, and the eigenvectors can be chosen to be orthogonal.\n",
    "# Example: Consider the symmetric matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. How do you find the eigenvalues of a matrix and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the eigenvalues of a matrix A, solve the characteristic equation\n",
    "#det (A - 位I) = 0\n",
    "#where 位 is the eigenvalue, I is the identity matrix, and det is the determinant.\n",
    "\n",
    "#The eigenvalues represent how much the corresponding eigenvectors are scaled when the matrix transformation is applied. Larger eigenvalues indicate that the corresponding eigenvectors are scaled more, while smaller eigenvalues indicate less scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What are eigenvectors and how are they related to eigenvalues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvectors are vectors that remain in the same direction after a linear transformation by a matrix. They are scaled by the corresponding eigenvalue, meaning:\n",
    "# Av =位v\n",
    "#where\n",
    "#A is the matrix\n",
    "#位 is the eigen value\n",
    "#v is the eigen vector\n",
    "#The relationship is that for each eigenvalue , there is a corresponding eigenvector v that only gets stretched or compressed, not rotated, by the transformation represented by A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The geometric interpretation of eigenvectors and eigenvalues is as follows:\n",
    "\n",
    "# Eigenvectors represent directions in space along which a linear transformation (matrix) only stretches or compresses the data, but does not rotate it.\n",
    "\n",
    "# Eigenvalues indicate the factor by which the eigenvector is stretched or compressed. For example, if \n",
    "# 位=2, the eigenvector is stretched by a factor of 2.\n",
    "\n",
    "# This interpretation is useful in understanding the effect of a transformation on the geometry of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What are some real-world applications of eigen decomposition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some real-world applications of eigen-decomposition include:\n",
    "\n",
    "# Principal Component Analysis (PCA): PCA uses eigen-decomposition to reduce dimensionality in datasets by finding principal components (eigenvectors) that capture the maximum variance in the data.\n",
    "\n",
    "# Quantum Mechanics: In quantum mechanics, eigenvalues represent the possible outcomes of measurements, such as energy levels of an atom.\n",
    "\n",
    "# Markov Chains: Eigen-decomposition is used in Markov Chains to compute steady-state probabilities.\n",
    "\n",
    "# Face Recognition: Eigen-decomposition is used in techniques like Eigenfaces for facial recognition by reducing the dimensionality of image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No, a matrix cannot have different sets of eigenvectors or eigenvalues. However, if a matrix has repeated eigenvalues (a degenerate case), there can be multiple eigenvectors corresponding to the same eigenvalue, forming a subspace of eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning?\n",
    "Discuss at least three specific applications or techniques that rely on Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigen-Decomposition.\n",
    "\n",
    "# Principal Component Analysis (PCA): Eigen-decomposition is used in PCA to find the principal components that capture the most variance in data, reducing dimensionality and helping with feature extraction and visualization.\n",
    "\n",
    "# Spectral Clustering: Eigen-decomposition is used in clustering algorithms like spectral clustering, where the eigenvectors of a similarity matrix are used to partition data into clusters.\n",
    "\n",
    "# Linear Discriminant Analysis (LDA): Eigen-decomposition is also used in LDA, where eigenvectors are used to find the directions that maximize class separability for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
