{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is hierarchical clustering, and how is it different from other clustering techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering is a clustering technique that builds a hierarchy of clusters either in a bottom-up (agglomerative) or top-down (divisive) manner. Unlike methods like K-means, it doesn't require the user to specify the number of clusters in advance.\n",
    "\n",
    "# Differences from other techniques:\n",
    "\n",
    "# Hierarchical clustering creates a tree-like structure (dendrogram), allowing exploration at different levels of granularity.\n",
    "\n",
    "# K-means assumes clusters are spherical and of equal size; hierarchical clustering doesn't.\n",
    "\n",
    "# Hierarchical clustering is deterministic, while K-means depends on random initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative Clustering (Bottom-Up):\n",
    "# Starts with each data point as its own cluster.\n",
    "# Iteratively merges the two closest clusters.\n",
    "# Continues until all points belong to a single cluster or a stopping criterion is met.\n",
    "\n",
    "# Divisive Clustering (Top-Down):\n",
    "# Starts with all data in a single cluster.\n",
    "# Recursively splits clusters into smaller ones.\n",
    "# Less commonly used due to higher computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the\n",
    "common distance metrics used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distance between two clusters is determined using linkage criteria, which define how distances between sets of observations are computed:\n",
    "# Single Linkage: Minimum distance between any two points from each cluster.\n",
    "# Complete Linkage: Maximum distance between any two points from each cluster.\n",
    "# Average Linkage: Average distance between all pairs of points from each cluster.\n",
    "# Ward’s Method: Minimizes the total within-cluster variance.\n",
    "\n",
    "# Common distance metrics include:\n",
    "# Euclidean distance (most common for numerical data).\n",
    "# Manhattan distance.\n",
    "# Cosine distance (for high-dimensional or sparse data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some\n",
    "common methods used for this purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common methods to determine the optimal number of clusters:\n",
    "\n",
    "# Dendrogram Cut:\n",
    "# Visually inspect the dendrogram and cut at the level where there’s a large jump in vertical distance.\n",
    "# The number of vertical lines cut at that level equals the number of clusters.\n",
    "\n",
    "# Elbow Method:\n",
    "# Plot the linkage distances at each merge step.\n",
    "# The “elbow” in the plot indicates a good number of clusters.\n",
    "\n",
    "# Silhouette Score:\n",
    "# Measures how similar an object is to its own cluster compared to others.\n",
    "# A higher average silhouette score indicates better clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dendrogram is a tree-like diagram that records the sequence of merges or splits in hierarchical clustering.\n",
    "\n",
    "# Usefulness:\n",
    "# Visualizes the clustering process.\n",
    "# Helps identify the number of natural clusters by cutting the dendrogram at a chosen height.\n",
    "# Reveals the level of similarity between points or clusters.\n",
    "# Can help detect outliers by identifying isolated branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the\n",
    "distance metrics different for each type of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, hierarchical clustering can be applied to both numerical and categorical data:\n",
    "\n",
    "# For numerical data: Use distance metrics like Euclidean, Manhattan, or Ward’s linkage.\n",
    "\n",
    "# For categorical data: Use measures like Hamming distance (number of mismatches), or convert data to a similarity matrix (e.g., using the Jaccard index).\n",
    "\n",
    "# Mixed data: Use Gower distance, which combines distances for both numerical and categorical variables.\n",
    "\n",
    "# The choice of distance metric depends on the data type to ensure meaningful similarity comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering can help identify outliers by:\n",
    "\n",
    "# Visual inspection of the dendrogram: Outliers often appear as data points that are merged at a much higher distance than the rest (they hang off isolated branches).\n",
    "\n",
    "# Analyzing cluster linkage distances: Points that join clusters at large distances can be flagged as anomalies.\n",
    "\n",
    "# Creating flat clusters: After cutting the dendrogram, check for very small clusters or singleton clusters—these may be outliers.\n",
    "\n",
    "# This method is unsupervised, so it provides a way to detect anomalies without labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
