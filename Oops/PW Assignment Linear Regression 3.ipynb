{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression is a regularized version of linear regression that adds an L2 penalty (squared magnitude of coefficients) to the loss function to reduce overfitting and handle multicollinearity.\n",
    "\n",
    "# Loss Function:\n",
    "# Ridge: Loss = RSS + λ * Σβ²\n",
    "# OLS: Loss = RSS\n",
    "\n",
    "# Difference:\n",
    "\n",
    "# OLS minimizes only the residual sum of squares (RSS).\n",
    "\n",
    "# Ridge adds a penalty term (controlled by λ) to shrink coefficients, reducing model complexity and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression shares the same basic assumptions as OLS regression:\n",
    "\n",
    "# 1.Linearity between predictors and response.\n",
    "# 2.Independence of observations.\n",
    "# 3.Homoscedasticity (constant variance of errors).\n",
    "# 4.Normality of residuals (for inference).\n",
    "# 5.No perfect multicollinearity (though Ridge can handle high multicollinearity better than OLS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda (λ) is selected using techniques like:\n",
    "\n",
    "# Cross-validation (usually k-fold): Try different values of λ and pick the one that minimizes validation error (e.g., RMSE).\n",
    "\n",
    "# Grid search: Test a range of λ values on a grid.\n",
    "\n",
    "# Automated tools: In Python, RidgeCV in sklearn can find the best λ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression cannot perform feature selection in the strict sense because it shrinks coefficients but does not set them to zero. All features are retained, though some may have very small coefficients.\n",
    "\n",
    "# For feature selection, Lasso Regression (L1 regularization) is more appropriate, as it can shrink coefficients exactly to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression handles multicollinearity well by shrinking correlated coefficients and stabilizing their values.\n",
    "# It reduces the variance caused by multicollinearity, leading to more reliable predictions, unlike OLS, which can have unstable and inflated coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, but categorical variables must be encoded first (e.g., using one-hot encoding or ordinal encoding) since Ridge Regression, like other linear models, requires numerical input.\n",
    "\n",
    "# Once encoded, Ridge treats them just like continuous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sign (positive/negative) of the coefficient indicates the direction of the relationship.\n",
    "\n",
    "# The magnitude is smaller than in OLS due to shrinkage, so it doesn’t reflect the actual effect size directly.\n",
    "\n",
    "# Interpretations should be relative: larger absolute values suggest stronger influence, but not exact impact due to regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, Ridge Regression can be used for time-series data after proper preprocessing:\n",
    "\n",
    "# Add lag features (e.g., previous values as predictors).\n",
    "\n",
    "# Encode seasonality or trend components.\n",
    "\n",
    "# Ensure data is stationary or trend-adjusted.\n",
    "\n",
    "# However, Ridge doesn't inherently account for time dependencies like ARIMA does, so it should be combined with time-series feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
